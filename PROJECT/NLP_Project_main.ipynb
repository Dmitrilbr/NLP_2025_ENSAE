{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e4bf8d-6ca3-4d77-98ed-a52e0c10715c",
   "metadata": {},
   "source": [
    "# NLP 2025 PROJECT : FAKE NEWS DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261f3763-3f25-4011-9a9b-5eecff1e7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a823ddd-caa0-443d-900a-d602777e862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e914c45f-9b99-4521-8303-91fd7e2e1e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241e2d0a-a8b7-45c9-a64f-45981a0fea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782502a9-9c69-4807-99bb-de68c89cfec9",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcce6dca-92fc-426c-ba25-dfb0022a4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_path = 'True.csv'\n",
    "fake_path = 'Fake.csv'\n",
    "\n",
    "true_df = pd.read_csv(true_path)\n",
    "fake_df = pd.read_csv(fake_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e985dc-3634-499c-923d-6eee2aaa866b",
   "metadata": {},
   "source": [
    "Adding a label and combining the two datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f6d6dcf-b03d-46e3-bc25-4f69dba65ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels 1 for real, 0 for fake\n",
    "true_df['label'] = 1\n",
    "fake_df['label'] = 0\n",
    "\n",
    "# Combining into one Dataframe\n",
    "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Only keeping first 10000 samples to speed up computations\n",
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1074b94-d0a0-4eb8-bc2d-baa5806d8116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f32f3d-2b4c-4016-86a1-94fada214ea7",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7cbb07e-ad1d-4e4a-a43f-8d6fdf0ffeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de54206-dcfe-46c4-8500-1e8814a72f1e",
   "metadata": {},
   "source": [
    "### 1) Heavy cleaning for Bag-of-words and TDIDF (term frequency-inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a47698e4-c703-48cb-8a8e-25de1ad68a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavy_cleaning(text):\n",
    "    # Putting all text in lowercase\n",
    "    text = text.lower()\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Removing Twitter handles\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Removing numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0afe54df-e198-4d86-b6ca-fcbef49d27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the cleaning\n",
    "df['text_heavy_cleaned'] = df['text'].apply(heavy_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb2c228-7592-499f-95e8-716b5b58c097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>text_heavy_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BREAKING: GOP Chairman Grassley Has Had Enoug...</td>\n",
       "      <td>Donald Trump s White House is in chaos, and th...</td>\n",
       "      <td>News</td>\n",
       "      <td>July 21, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>donald trump white house chaos trying cover ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Failed GOP Candidates Remembered In Hilarious...</td>\n",
       "      <td>Now that Donald Trump is the presumptive GOP n...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 7, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>donald trump presumptive gop nominee time reme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike Pence’s New DC Neighbors Are HILARIOUSLY...</td>\n",
       "      <td>Mike Pence is a huge homophobe. He supports ex...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 3, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>mike penny huge homophobe support exgay conver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California AG pledges to defend birth control ...</td>\n",
       "      <td>SAN FRANCISCO (Reuters) - California Attorney ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>October 6, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>san francisco reuters california attorney gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AZ RANCHERS Living On US-Mexico Border Destroy...</td>\n",
       "      <td>Twisted reasoning is all that comes from Pelos...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Apr 25, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>twisted reasoning come pelosi day especially p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   BREAKING: GOP Chairman Grassley Has Had Enoug...   \n",
       "1   Failed GOP Candidates Remembered In Hilarious...   \n",
       "2   Mike Pence’s New DC Neighbors Are HILARIOUSLY...   \n",
       "3  California AG pledges to defend birth control ...   \n",
       "4  AZ RANCHERS Living On US-Mexico Border Destroy...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Donald Trump s White House is in chaos, and th...          News   \n",
       "1  Now that Donald Trump is the presumptive GOP n...          News   \n",
       "2  Mike Pence is a huge homophobe. He supports ex...          News   \n",
       "3  SAN FRANCISCO (Reuters) - California Attorney ...  politicsNews   \n",
       "4  Twisted reasoning is all that comes from Pelos...      politics   \n",
       "\n",
       "               date  label                                 text_heavy_cleaned  \n",
       "0     July 21, 2017      0  donald trump white house chaos trying cover ru...  \n",
       "1       May 7, 2016      0  donald trump presumptive gop nominee time reme...  \n",
       "2  December 3, 2016      0  mike penny huge homophobe support exgay conver...  \n",
       "3  October 6, 2017       1  san francisco reuters california attorney gene...  \n",
       "4      Apr 25, 2017      0  twisted reasoning come pelosi day especially p...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9b7e4-b7e3-4850-bc20-1f458f01f40b",
   "metadata": {},
   "source": [
    "### 2) Light cleaning for Word2Vec and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b013d3af-7e07-4ec3-8bae-c006f8550a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_cleaning(text):\n",
    "    # putting all text in lowercase\n",
    "    text = text.lower()\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Removing Twitter handles\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Removing excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e582953-65ea-4c4e-a29a-8b38998fa454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the cleaning\n",
    "df['text_light_cleaned'] = df['text'].apply(light_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ecd323e-e799-4085-9728-a961b3d73150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>text_heavy_cleaned</th>\n",
       "      <th>text_light_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BREAKING: GOP Chairman Grassley Has Had Enoug...</td>\n",
       "      <td>Donald Trump s White House is in chaos, and th...</td>\n",
       "      <td>News</td>\n",
       "      <td>July 21, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>donald trump white house chaos trying cover ru...</td>\n",
       "      <td>donald trump s white house is in chaos, and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Failed GOP Candidates Remembered In Hilarious...</td>\n",
       "      <td>Now that Donald Trump is the presumptive GOP n...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 7, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>donald trump presumptive gop nominee time reme...</td>\n",
       "      <td>now that donald trump is the presumptive gop n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike Pence’s New DC Neighbors Are HILARIOUSLY...</td>\n",
       "      <td>Mike Pence is a huge homophobe. He supports ex...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 3, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>mike penny huge homophobe support exgay conver...</td>\n",
       "      <td>mike pence is a huge homophobe. he supports ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California AG pledges to defend birth control ...</td>\n",
       "      <td>SAN FRANCISCO (Reuters) - California Attorney ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>October 6, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>san francisco reuters california attorney gene...</td>\n",
       "      <td>san francisco (reuters) - california attorney ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AZ RANCHERS Living On US-Mexico Border Destroy...</td>\n",
       "      <td>Twisted reasoning is all that comes from Pelos...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Apr 25, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>twisted reasoning come pelosi day especially p...</td>\n",
       "      <td>twisted reasoning is all that comes from pelos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   BREAKING: GOP Chairman Grassley Has Had Enoug...   \n",
       "1   Failed GOP Candidates Remembered In Hilarious...   \n",
       "2   Mike Pence’s New DC Neighbors Are HILARIOUSLY...   \n",
       "3  California AG pledges to defend birth control ...   \n",
       "4  AZ RANCHERS Living On US-Mexico Border Destroy...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Donald Trump s White House is in chaos, and th...          News   \n",
       "1  Now that Donald Trump is the presumptive GOP n...          News   \n",
       "2  Mike Pence is a huge homophobe. He supports ex...          News   \n",
       "3  SAN FRANCISCO (Reuters) - California Attorney ...  politicsNews   \n",
       "4  Twisted reasoning is all that comes from Pelos...      politics   \n",
       "\n",
       "               date  label                                 text_heavy_cleaned  \\\n",
       "0     July 21, 2017      0  donald trump white house chaos trying cover ru...   \n",
       "1       May 7, 2016      0  donald trump presumptive gop nominee time reme...   \n",
       "2  December 3, 2016      0  mike penny huge homophobe support exgay conver...   \n",
       "3  October 6, 2017       1  san francisco reuters california attorney gene...   \n",
       "4      Apr 25, 2017      0  twisted reasoning come pelosi day especially p...   \n",
       "\n",
       "                                  text_light_cleaned  \n",
       "0  donald trump s white house is in chaos, and th...  \n",
       "1  now that donald trump is the presumptive gop n...  \n",
       "2  mike pence is a huge homophobe. he supports ex...  \n",
       "3  san francisco (reuters) - california attorney ...  \n",
       "4  twisted reasoning is all that comes from pelos...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104cf1d-3e30-4e42-820a-4f84baad909c",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ec8c6d-e681-4a71-a297-d4a5a0dfe5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag, word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92170e-8a95-4765-9f0f-879287968307",
   "metadata": {},
   "source": [
    "### 1) Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a5ee4de-d7de-4a09-8553-e6d01424d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using heavy cleaned text\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)  # Limit vocab size for efficiency\n",
    "X_bow = bow_vectorizer.fit_transform(df['text_heavy_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a37700be-0bee-49ca-861e-aa208f0b0617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1279672 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3a317-f14c-4d81-84a7-c313c6f8a8a2",
   "metadata": {},
   "source": [
    "### 2) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d67e7739-fd53-426f-90f4-54da347b79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['text_heavy_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5e1016f-1b1e-49d6-9cfc-23eb93815358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1279672 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdfa95-8036-421a-9f0b-93d91023d5e8",
   "metadata": {},
   "source": [
    "### 3) Word-to-Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "583c2f30-829f-44c2-b742-39cd5d21b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the light-cleaned text for Word2Vec\n",
    "df['tokens'] = df['text_light_cleaned'].apply(word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model on your corpus\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=df['tokens'],\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Function to get average Word2Vec embedding for a document, to prepare the input for the machine learning model\n",
    "def get_avg_word2vec(tokens, model, vector_size=100):\n",
    "    vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(vector_size)\n",
    "\n",
    "# Apply to your dataset\n",
    "X_w2v = np.vstack(df['tokens'].apply(lambda tokens: get_avg_word2vec(tokens, w2v_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4529eaec-6ca3-4834-a45d-86a022578206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b507470-6787-48fe-9c7c-05c2b2b01dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c0636-d17e-4a6e-9ae8-9791ae495fb5",
   "metadata": {},
   "source": [
    "### 4) BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d691ae3e-36ba-4c6c-86ec-8478e57f794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hf_xet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b61f0c57-3bed-4751-bc33-aef84c32d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(embeddings)\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e21d6a9b-fe64-49be-b669-8577202073fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [06:14<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "X_bert = get_bert_embeddings(df['text_light_cleaned'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eeff5b-55d0-4fd7-88fd-82536f4852e5",
   "metadata": {},
   "source": [
    "### 5) Linguistic cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4487657e-6216-4b76-9256-a7b171d3a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "905c52d0-445a-49aa-b62b-213e294559a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Defining linguistic categories according to table 1 from the article\n",
    "PERSONAL_PRONOUNS = {\"i\", \"we\", \"she\", \"him\", \"me\", \"us\", \"her\", \"he\"}\n",
    "FIRST_PERSON_SINGULAR = {\"i\", \"me\"}\n",
    "FIRST_PERSON_PLURAL = {\"we\", \"us\"}\n",
    "SECOND_PERSON = {\"you\", \"your\"}\n",
    "THIRD_PERSON_SINGULAR = {\"she\", \"he\", \"her\", \"him\"}\n",
    "IMPERSONAL_PRONOUNS = {\"it\", \"that\", \"anything\", \"everything\", \"something\"}\n",
    "ARTICLES = {\"a\", \"an\", \"the\"}\n",
    "PREPOSITIONS = {\"above\", \"below\", \"near\", \"under\", \"over\", \"behind\", \"beyond\", \"through\", \"among\", \"within\", \"without\", \"across\", \"against\", \"along\", \"around\", \"at\", \"before\", \"by\", \"during\", \"except\", \"for\", \"from\", \"in\", \"into\", \"of\", \"off\", \"on\", \"to\", \"until\", \"up\", \"with\"}\n",
    "AUXILIARY_VERBS = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\"}\n",
    "COMMON_ADVERBS = {\"just\", \"usually\", \"even\", \"only\", \"also\", \"still\", \"already\"}\n",
    "CONJUNCTIONS = {\"and\", \"but\", \"or\", \"yet\", \"so\", \"for\", \"nor\", \"although\", \"because\", \"since\", \"unless\", \"until\", \"while\"}\n",
    "NEGATIONS = {\"no\", \"not\", \"never\", \"none\", \"nothing\", \"nowhere\"}\n",
    "COMMON_VERBS = {\n",
    "    \"run\", \"walk\", \"swim\", \"go\", \"come\", \"make\", \"do\", \"say\", \"get\", \n",
    "    \"see\", \"know\", \"think\", \"be\", \"have\", \"take\", \"look\", \"want\", \n",
    "    \"give\", \"use\", \"find\", \"tell\", \"ask\", \"work\", \"seem\", \"feel\", \n",
    "    \"try\", \"leave\", \"call\"\n",
    "}   # according to https://www.englishclub.com/vocabulary/common-verbs-25.php\n",
    "\n",
    "COMMON_ADJECTIVES = {\n",
    "    \"better\", \"greater\", \"larger\", \"good\", \"bad\", \"happy\", \"sad\", \n",
    "    \"new\", \"old\", \"young\", \"small\", \"big\", \"first\", \"last\", \"long\", \n",
    "    \"great\", \"little\", \"own\", \"other\", \"right\", \"high\", \"different\", \n",
    "    \"large\", \"next\", \"early\", \"important\", \"few\", \"public\", \"same\", \"able\"\n",
    "}   # according to https://www.englishclub.com/vocabulary/common-adjectives-25.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9185218-5815-4032-9150-555a7d049fcf",
   "metadata": {},
   "source": [
    "Extracting main linguistic features, sentiment analysis and a few others have been commented to make the code run in decent time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17f5f3b4-2596-45f4-aa78-f2f432bec1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_linguistic_features(text):\n",
    "    features = {}\n",
    "    # blob = TextBlob(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tags = pos_tag(tokens)\n",
    "    token_count = Counter(tokens)\n",
    "    total_words = len(tokens)\n",
    "    total_chars = len(text)\n",
    "    # total_sentences = len(blob.sentences)\n",
    "    features['char_count'] = total_chars\n",
    "    features['word_count'] = total_words\n",
    "    # features['sentence_count'] = total_sentences\n",
    "    # features['syllables_count'] = textstat.syllable_count(text)\n",
    "    # features['words_per_sentence'] = total_words / total_sentences if total_sentences > 0 else 0\n",
    "    features['long_words_count'] = sum(1 for w in tokens if len(w) > 6)\n",
    "    features['all_caps_count'] = sum(1 for w in tokens if w.isupper())\n",
    "    features['unique_words_count'] = len(set(tokens))\n",
    "    features['avg_word_length'] = np.mean([len(w) for w in tokens]) if tokens else 0\n",
    "\n",
    "    # Percentage-based linguistic categories\n",
    "    def count_percent(word_set):\n",
    "        return sum(1 for w in tokens if w in word_set) / total_words if total_words > 0 else 0\n",
    "\n",
    "    features['personal_pronouns_pct'] = count_percent(PERSONAL_PRONOUNS)\n",
    "    features['first_person_singular_pct'] = count_percent(FIRST_PERSON_SINGULAR)\n",
    "    features['first_person_plural_pct'] = count_percent(FIRST_PERSON_PLURAL)\n",
    "    features['second_person_pct'] = count_percent(SECOND_PERSON)\n",
    "    features['third_person_singular_pct'] = count_percent(THIRD_PERSON_SINGULAR)\n",
    "    features['impersonal_pronouns_pct'] = count_percent(IMPERSONAL_PRONOUNS)\n",
    "    features['articles_pct'] = count_percent(ARTICLES)\n",
    "    features['prepositions_pct'] = count_percent(PREPOSITIONS)\n",
    "    features['auxiliary_verbs_pct'] = count_percent(AUXILIARY_VERBS)\n",
    "    features['common_adverbs_pct'] = count_percent(COMMON_ADVERBS)\n",
    "    features['conjunctions_pct'] = count_percent(CONJUNCTIONS)\n",
    "    features['negations_pct'] = count_percent(NEGATIONS)\n",
    "    features['common_verbs_pct'] = count_percent(COMMON_VERBS)\n",
    "    features['common_adjectives_pct'] = count_percent(COMMON_ADJECTIVES)\n",
    "\n",
    "    # POS Tag counts\n",
    "    features['noun_count'] = sum(1 for _, tag in tags if tag.startswith('NN'))\n",
    "    features['pronoun_count'] = sum(1 for _, tag in tags if tag.startswith('PRP'))\n",
    "\n",
    "    # Sentiment\n",
    "    # features['sentiment_polarity'] = blob.sentiment.polarity\n",
    "\n",
    "    # Punctuation\n",
    "    features['punctuation_count'] = sum(1 for c in text if c in string.punctuation)\n",
    "    features['fullstop_count'] = text.count('.')\n",
    "    features['comma_count'] = text.count(',')\n",
    "    features['colon_count'] = text.count(':')\n",
    "    features['semicolon_count'] = text.count(';')\n",
    "    features['question_mark_count'] = text.count('?')\n",
    "    features['exclamation_mark_count'] = text.count('!')\n",
    "    features['dash_count'] = text.count('-')\n",
    "    features['apostrophe_count'] = text.count(\"'\")\n",
    "    features['brackets_count'] = text.count('(') + text.count(')')\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Applying to our data\n",
    "linguistic_df = df['text'].apply(extract_linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65949500-38ed-4de4-9a69-ffb2701af24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>long_words_count</th>\n",
       "      <th>all_caps_count</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>personal_pronouns_pct</th>\n",
       "      <th>first_person_singular_pct</th>\n",
       "      <th>first_person_plural_pct</th>\n",
       "      <th>second_person_pct</th>\n",
       "      <th>...</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>fullstop_count</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>dash_count</th>\n",
       "      <th>apostrophe_count</th>\n",
       "      <th>brackets_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2114.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>4.348259</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2823.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>4.005172</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>4.703271</td>\n",
       "      <td>0.021028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>629.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>793.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4.510345</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   char_count  word_count  long_words_count  all_caps_count  \\\n",
       "0      2114.0       402.0              73.0             0.0   \n",
       "1      2823.0       580.0              87.0             0.0   \n",
       "2      2402.0       428.0             108.0             0.0   \n",
       "3       629.0       105.0              36.0             0.0   \n",
       "4       793.0       145.0              29.0             0.0   \n",
       "\n",
       "   unique_words_count  avg_word_length  personal_pronouns_pct  \\\n",
       "0               208.0         4.348259               0.007463   \n",
       "1               298.0         4.005172               0.034483   \n",
       "2               236.0         4.703271               0.021028   \n",
       "3                75.0         5.133333               0.009524   \n",
       "4                99.0         4.510345               0.027586   \n",
       "\n",
       "   first_person_singular_pct  first_person_plural_pct  second_person_pct  ...  \\\n",
       "0                   0.000000                 0.002488           0.000000  ...   \n",
       "1                   0.003448                 0.010345           0.005172  ...   \n",
       "2                   0.000000                 0.000000           0.000000  ...   \n",
       "3                   0.000000                 0.000000           0.000000  ...   \n",
       "4                   0.000000                 0.000000           0.000000  ...   \n",
       "\n",
       "   punctuation_count  fullstop_count  comma_count  colon_count  \\\n",
       "0               50.0            16.0         26.0          2.0   \n",
       "1               93.0            21.0         44.0         10.0   \n",
       "2               72.0            21.0         22.0          2.0   \n",
       "3               10.0             3.0          4.0          0.0   \n",
       "4               10.0             5.0          4.0          0.0   \n",
       "\n",
       "   semicolon_count  question_mark_count  exclamation_mark_count  dash_count  \\\n",
       "0              0.0                  0.0                     0.0         3.0   \n",
       "1              0.0                  3.0                     1.0         4.0   \n",
       "2              0.0                  0.0                     0.0        10.0   \n",
       "3              0.0                  0.0                     0.0         1.0   \n",
       "4              0.0                  0.0                     0.0         1.0   \n",
       "\n",
       "   apostrophe_count  brackets_count  \n",
       "0               0.0             2.0  \n",
       "1               1.0             4.0  \n",
       "2               0.0             4.0  \n",
       "3               0.0             2.0  \n",
       "4               0.0             0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linguistic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac21e3-1d90-4855-ba3a-0681aee7788d",
   "metadata": {},
   "source": [
    "## Testing our model of each feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3075a9b2-66f5-4626-8412-7bf68b59eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7daadfc-22ec-44f7-b595-c6a7bcc5703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with Bag of Words features ---\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1978           10.34s\n",
      "         2           1.0453            9.86s\n",
      "         3           0.9182            9.39s\n",
      "         4           0.8108            9.03s\n",
      "         5           0.7191            8.83s\n",
      "         6           0.6399            8.65s\n",
      "         7           0.5715            8.61s\n",
      "         8           0.5117            8.47s\n",
      "         9           0.4595            8.32s\n",
      "        10           0.4131            8.25s\n",
      "        20           0.1602            7.22s\n",
      "        30           0.0763            6.30s\n",
      "        40           0.0467            5.31s\n",
      "        50           0.0407            4.23s\n",
      "        60           0.0378            3.28s\n",
      "        70           0.0324            2.40s\n",
      "        80           0.0301            1.57s\n",
      "        90           0.0275            0.78s\n",
      "       100           0.0247            0.00s\n",
      "Saved model: models/gbc_bag_of_words.joblib\n",
      "\n",
      "--- Training with TF-IDF features ---\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1978           47.90s\n",
      "         2           1.0453           47.27s\n",
      "         3           0.9181           45.91s\n",
      "         4           0.8106           44.89s\n",
      "         5           0.7188           43.89s\n",
      "         6           0.6399           42.96s\n",
      "         7           0.5709           42.37s\n",
      "         8           0.5114           41.71s\n",
      "         9           0.4585           41.02s\n",
      "        10           0.4125           40.33s\n",
      "        20           0.1595           35.39s\n",
      "        30           0.0740           31.12s\n",
      "        40           0.0435           26.56s\n",
      "        50           0.0359           22.12s\n",
      "        60           0.0318           17.55s\n",
      "        70           0.0298           13.13s\n",
      "        80           0.0270            8.74s\n",
      "        90           0.0250            4.36s\n",
      "       100           0.0226            0.00s\n",
      "Saved model: models/gbc_tf-idf.joblib\n",
      "\n",
      "--- Training with Word2Vec features ---\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2445           34.58s\n",
      "         2           1.1315           33.62s\n",
      "         3           1.0337           33.53s\n",
      "         4           0.9507           33.32s\n",
      "         5           0.8785           32.94s\n",
      "         6           0.8166           32.38s\n",
      "         7           0.7623           31.91s\n",
      "         8           0.7133           31.49s\n",
      "         9           0.6677           31.20s\n",
      "        10           0.6292           30.78s\n",
      "        20           0.3719           27.18s\n",
      "        30           0.2476           23.95s\n",
      "        40           0.1797           20.44s\n",
      "        50           0.1384           17.07s\n",
      "        60           0.1123           13.67s\n",
      "        70           0.0934           10.23s\n",
      "        80           0.0799            6.82s\n",
      "        90           0.0692            3.41s\n",
      "       100           0.0601            0.00s\n",
      "Saved model: models/gbc_word2vec.joblib\n",
      "\n",
      "--- Training with BERT features ---\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2468            4.39m\n",
      "         2           1.1324            4.45m\n",
      "         3           1.0367            4.40m\n",
      "         4           0.9538            4.33m\n",
      "         5           0.8831            4.31m\n",
      "         6           0.8200            4.28m\n",
      "         7           0.7642            4.21m\n",
      "         8           0.7136            4.16m\n",
      "         9           0.6688            4.12m\n",
      "        10           0.6272            4.07m\n",
      "        20           0.3802            3.58m\n",
      "        30           0.2710            3.13m\n",
      "        40           0.2076            2.68m\n",
      "        50           0.1673            2.25m\n",
      "        60           0.1392            1.80m\n",
      "        70           0.1191            1.35m\n",
      "        80           0.1037           54.08s\n",
      "        90           0.0925           27.07s\n",
      "       100           0.0835            0.00s\n",
      "Saved model: models/gbc_bert.joblib\n",
      "\n",
      "--- Training with Linguistic features ---\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2507            5.28s\n",
      "         2           1.1431            5.32s\n",
      "         3           1.0508            5.36s\n",
      "         4           0.9726            5.14s\n",
      "         5           0.9069            5.03s\n",
      "         6           0.8504            4.94s\n",
      "         7           0.7995            4.86s\n",
      "         8           0.7557            4.76s\n",
      "         9           0.7161            4.70s\n",
      "        10           0.6818            4.65s\n",
      "        20           0.4674            4.12s\n",
      "        30           0.3702            3.57s\n",
      "        40           0.3191            3.07s\n",
      "        50           0.2856            2.54s\n",
      "        60           0.2610            2.03s\n",
      "        70           0.2419            1.51s\n",
      "        80           0.2259            1.01s\n",
      "        90           0.2142            0.50s\n",
      "       100           0.2023            0.00s\n",
      "Saved model: models/gbc_linguistic.joblib\n",
      "\n",
      "=== Model Evaluation Results ===\n",
      "              Accuracy  Precision  Recall  F1 Score\n",
      "Bag of Words    0.9935     0.9900  0.9970    0.9935\n",
      "TF-IDF          0.9925     0.9880  0.9970    0.9925\n",
      "Word2Vec        0.9855     0.9830  0.9879    0.9854\n",
      "BERT            0.9720     0.9718  0.9718    0.9718\n",
      "Linguistic      0.9595     0.9635  0.9547    0.9591\n"
     ]
    }
   ],
   "source": [
    "y = df['label']\n",
    "\n",
    "# Define feature sets\n",
    "feature_sets = {\n",
    "    \"Bag of Words\": X_bow,\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"Word2Vec\": X_w2v,\n",
    "    \"BERT\": X_bert,\n",
    "    \"Linguistic\": linguistic_df\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, X in feature_sets.items():\n",
    "    print(f\"\\n--- Training with {name} features ---\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Model training\n",
    "    clf = GradientBoostingClassifier(verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Create a directory to save and store models\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    model_path = f\"models/gbc_{name.replace(' ', '_').lower()}.joblib\"\n",
    "    joblib.dump(clf, model_path)\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n=== Model Evaluation Results ===\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e5062-a900-43e0-aca6-22f646fd22dd",
   "metadata": {},
   "source": [
    "## Testing generalizability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db02dcc-3dfa-4132-8078-30011136078e",
   "metadata": {},
   "source": [
    "Using the fake or real news dataset that can be found here : https://www.kaggle.com/datasets/jillanisofttech/fake-or-real-news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680cbc5-98e2-4a34-a11f-09fdc0b11404",
   "metadata": {},
   "source": [
    "Loading models if needed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ff10afc-e54e-4e60-8bf3-30ce1b3f906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: models/gbc_linguistic.joblib\n"
     ]
    }
   ],
   "source": [
    "# model_names = [\"bag_of_words\", \"tf-idf\", \"word2vec\", \"bert\", \"linguistic\"]\n",
    "name = \"linguistic\"\n",
    "loaded_models = {}\n",
    "path = f\"models/gbc_{name}.joblib\"\n",
    "loaded_models[name] = joblib.load(path)\n",
    "print(f\"Loaded model: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31d0686e-57e7-46fd-951a-f4fb14cb44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_path = 'fake_or_real_news.csv'\n",
    "\n",
    "new_data = pd.read_csv(new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b17b610-219b-4473-9a70-7f4e4c045c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6335, 4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45ebd993-ac96-4327-8771-2293b87c45d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb6f9c2b-87a0-490b-a713-6d73b75fbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['text_heavy_cleaned'] = new_data['text'].apply(heavy_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c4b389df-9f27-4d0c-94b0-aa903682e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['text_light_cleaned'] = new_data['text'].apply(light_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dcb2cb6a-f16b-49a1-a0a3-8c5ce74e5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['label_numeric'] = new_data['label'].map({'REAL': 1, 'FAKE': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58a37609-4da4-4b8b-b591-3d3dd0c1156e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_heavy_cleaned</th>\n",
       "      <th>text_light_cleaned</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>daniel greenfield shillman journalism fellow f...</td>\n",
       "      <td>daniel greenfield, a shillman journalism fello...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>u secretary state john f kerry said monday sto...</td>\n",
       "      <td>u.s. secretary of state john f. kerry said mon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>— kaydee king november lesson tonight dem loss...</td>\n",
       "      <td>— kaydee king () november 9, 2016 the lesson f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>primary day new york frontrunners hillary clin...</td>\n",
       "      <td>it's primary day in new york and front-runners...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                  text_heavy_cleaned  \\\n",
       "0  daniel greenfield shillman journalism fellow f...   \n",
       "1  google pinterest digg linkedin reddit stumbleu...   \n",
       "2  u secretary state john f kerry said monday sto...   \n",
       "3  — kaydee king november lesson tonight dem loss...   \n",
       "4  primary day new york frontrunners hillary clin...   \n",
       "\n",
       "                                  text_light_cleaned  label_numeric  \n",
       "0  daniel greenfield, a shillman journalism fello...              0  \n",
       "1  google pinterest digg linkedin reddit stumbleu...              0  \n",
       "2  u.s. secretary of state john f. kerry said mon...              1  \n",
       "3  — kaydee king () november 9, 2016 the lesson f...              0  \n",
       "4  it's primary day in new york and front-runners...              1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3255f-f556-4464-8df0-f1f856a1d287",
   "metadata": {},
   "source": [
    "### 1) Testing generalizability of bag_of_words based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "af9b19d0-b240-41d1-b43b-6f9f0ce2761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using heavy cleaned text\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)  # Limit vocab size for efficiency\n",
    "X_bow_new = bow_vectorizer.fit_transform(new_data['text_heavy_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cde1dedf-d2bb-42fd-85cd-affaf096a5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on new data (X_bow_new):\n",
      "Accuracy : 0.5122\n",
      "Precision: 0.6738\n",
      "Recall   : 0.0495\n",
      "F1 Score : 0.0922\n"
     ]
    }
   ],
   "source": [
    "y_true_new = new_data['label_numeric']\n",
    "y_pred_new = loaded_models[\"bag_of_words\"].predict(X_bow_new)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_true_new, y_pred_new)\n",
    "precision = precision_score(y_true_new, y_pred_new)\n",
    "recall = recall_score(y_true_new, y_pred_new)\n",
    "f1 = f1_score(y_true_new, y_pred_new)\n",
    "\n",
    "print(\"Evaluation on new data (X_bow_new):\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61fc70-4f33-4d4e-b246-9b520f21c6b0",
   "metadata": {},
   "source": [
    "### 2) Testing generalizability of TF-IDF based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9af2bc6-0b41-4b3c-b887-3db886844653",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf_new = tfidf_vectorizer.fit_transform(new_data['text_heavy_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d4b4d08-33f6-4fe3-8460-f0b2c9f57c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on new data (X_bow_new):\n",
      "Accuracy : 0.5121\n",
      "Precision: 0.6835\n",
      "Recall   : 0.0470\n",
      "F1 Score : 0.0879\n"
     ]
    }
   ],
   "source": [
    "y_true_new = new_data['label_numeric']\n",
    "y_pred_new = loaded_models[\"tf-idf\"].predict(X_tfidf_new)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_true_new, y_pred_new)\n",
    "precision = precision_score(y_true_new, y_pred_new)\n",
    "recall = recall_score(y_true_new, y_pred_new)\n",
    "f1 = f1_score(y_true_new, y_pred_new)\n",
    "\n",
    "print(\"Evaluation on new data (X_tfidf_new):\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4662c-56ec-48fa-8e4e-f8de57733087",
   "metadata": {},
   "source": [
    "### 3) Testing generalizability of Word-to-vec based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ee7a9b0-335a-4a70-aa29-bde051c281ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the light-cleaned text for Word2Vec\n",
    "new_data['tokens'] = new_data['text_light_cleaned'].apply(word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model on your corpus\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=df['tokens'],\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Function to get average Word2Vec embedding for a document, to prepare the input for the machine learning model\n",
    "def get_avg_word2vec(tokens, model, vector_size=100):\n",
    "    vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(vector_size)\n",
    "\n",
    "# Apply to your dataset\n",
    "X_w2v_new = np.vstack(new_data['tokens'].apply(lambda tokens: get_avg_word2vec(tokens, w2v_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3866bcd3-6aa9-4a3d-9099-8e19d81da3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on new data (X_w2v_new):\n",
      "Accuracy : 0.5001\n",
      "Precision: 1.0000\n",
      "Recall   : 0.0013\n",
      "F1 Score : 0.0025\n"
     ]
    }
   ],
   "source": [
    "y_true_new = new_data['label_numeric']\n",
    "y_pred_new = loaded_models[\"word2vec\"].predict(X_w2v_new)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_true_new, y_pred_new)\n",
    "precision = precision_score(y_true_new, y_pred_new)\n",
    "recall = recall_score(y_true_new, y_pred_new)\n",
    "f1 = f1_score(y_true_new, y_pred_new)\n",
    "\n",
    "print(\"Evaluation on new data (X_w2v_new):\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21339ce3-8108-44f2-954c-b385cbfbb640",
   "metadata": {},
   "source": [
    "### 4) Testing generalizability of BERT based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "55b6afad-f093-476e-8962-1d6488be8480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [04:13<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "X_bert_new = get_bert_embeddings(new_data['text_light_cleaned'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4677c4fc-34fd-46dc-917a-f36aff3453c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on new data (X_bert_new):\n",
      "Accuracy : 0.6586\n",
      "Precision: 0.7238\n",
      "Recall   : 0.5140\n",
      "F1 Score : 0.6011\n"
     ]
    }
   ],
   "source": [
    "y_true_new = new_data['label_numeric']\n",
    "y_pred_new = loaded_models[\"bert\"].predict(X_bert_new)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_true_new, y_pred_new)\n",
    "precision = precision_score(y_true_new, y_pred_new)\n",
    "recall = recall_score(y_true_new, y_pred_new)\n",
    "f1 = f1_score(y_true_new, y_pred_new)\n",
    "\n",
    "print(\"Evaluation on new data (X_bert_new):\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845cd1-42b3-43d1-8db3-33d5732aed6f",
   "metadata": {},
   "source": [
    "### 5) Testing generalizability of Linguistic cues based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b555577c-538a-4789-aa18-5f381ff86879",
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_df = new_data['text'].apply(extract_linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "87b041fa-b765-41ee-96f9-0795b46d7f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on new data (linguistic_df):\n",
      "Accuracy : 0.5634\n",
      "Precision: 0.6576\n",
      "Recall   : 0.2665\n",
      "F1 Score : 0.3793\n"
     ]
    }
   ],
   "source": [
    "y_true_new = new_data['label_numeric']\n",
    "y_pred_new = loaded_models[\"linguistic\"].predict(linguistic_df)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_true_new, y_pred_new)\n",
    "precision = precision_score(y_true_new, y_pred_new)\n",
    "recall = recall_score(y_true_new, y_pred_new)\n",
    "f1 = f1_score(y_true_new, y_pred_new)\n",
    "\n",
    "print(\"Evaluation on new data (linguistic_df):\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
